
# Ridge & Lasso Regression

Ridge and Lasso regression are powerful techniques generally used for creating parsimonious models in presence of a ‘large’ number of features. Here ‘large’ can typically mean either of two things:

1. Large enough to enhance the tendency of a model to overfit (as low as 10 variables might cause overfitting)
2. Large enough to cause computational challenges. With modern systems, this situation might arise in case of millions or billions of features

Though Ridge and Lasso might appear to work towards a common goal, the inherent properties and practical use cases differ substantially. They use ‘regularization’ techniques which work by penalizing the magnitude of coefficients of features along with minimizing the error between predicted and actual observations. The key difference is in how they assign penalty to the coefficients.

**Ridge Regression:**

It performs L2 regularization which adds penalty equivalent to square of the magnitude of coefficients
Minimization objective 

`LS Obj + α * (sum of square of coefficients)`

**Lasso Regression:**

It performs L1 regularization which adds penalty equivalent to absolute value of the magnitude of coefficients
Minimization objective

`LS Obj + α * (sum of absolute value of coefficients)`

*** ‘LS Obj’ refers to ‘least squares objective’, i.e. the linear regression objective without regularization.

## Ridge Regression

Ridge Regression is a technique used when the data suffers from multicollinearity (independent variables are highly correlated). In multicollinearity, even though the least squares estimates (OLS) are unbiased, their variances are large which deviates the observed value far from the true value. By adding a degree of bias to the regression estimates, ridge regression reduces the standard errors.

Linear regression can be represented as:

```
y = a + b*x
```

**Collinearity** is a linear association between two explanatory variables. Two variables are perfectly collinear if there is an exact linear relationship between them. For example, X<sub>2i</sub> and X<sub>1i</sub> are perfectly collinear if there exist parameters β0 and β1 such that, for all observations i, we have
 
 X<sub>2i</sub> = β0 + β1X<sub>1i</sub>

Multicollinearity refers to a situation in which two or more explanatory variables in a multiple regression model are highly linearly related. We have perfect multicollinearity if, for example as in the equation above, the correlation between two independent variables is equal to 1 or −1. In practice, we rarely face perfect multicollinearity in a data set. More commonly, the issue of multicollinearity arises when there is an approximate linear relationship among two or more independent variables.

Y<sub>i</sub> = β0 + β<sub>1</sub>X<sub>1i</sub> + β<sub>2</sub>X<sub>2i</sub> + β<sub>3</sub>X<sub>3i</sub> + ... + + β<sub>n</sub>X<sub>ni</sub> = 0 

### Important Points:

- The assumptions of this regression is same as least squared regression except normality is not to be assumed
- It shrinks the value of coefficients but doesn’t reaches zero, which suggests no feature selection feature
- This is a regularization method and uses l2 regularization.


```python
import numpy as np
import pandas as pd
import random
import matplotlib.pyplot as plt
%matplotlib inline
from matplotlib.pylab import rcParams
rcParams['figure.figsize'] = 12, 10

#Define input array with angles from 60deg to 300deg converted to radians
x = np.array([i*np.pi/180 for i in range(60,300,4)])
np.random.seed(10)  #Setting seed for reproducability
y = np.sin(x) + np.random.normal(0,0.15,len(x))
data = pd.DataFrame(np.column_stack([x,y]),columns=['x','y'])
plt.plot(data['x'],data['y'],'.')
```




    [<matplotlib.lines.Line2D at 0x7f1e320a2c88>]




![png](3.%20Ridge%20Regression_files/3.%20Ridge%20Regression_12_1.png)


This resembles a sine curve but not exactly because of the noise. We’ll use this as an example to test different scenarios in this article. Lets try to estimate the sine function using polynomial regression with powers of x form 1 to 15. Lets add a column for each power upto 15 in our dataframe. This can be accomplished using the following code:




```python
for i in range(2,16):  #power of 1 is already there
    colname = 'x_%d'%i      #new var will be x_power
    data[colname] = data['x']**i
print( data.head())
```

              x         y       x_2       x_3       x_4       x_5       x_6  \
    0  1.047198  1.065763  1.096623  1.148381  1.202581  1.259340  1.318778   
    1  1.117011  1.006086  1.247713  1.393709  1.556788  1.738948  1.942424   
    2  1.186824  0.695374  1.408551  1.671702  1.984016  2.354677  2.794587   
    3  1.256637  0.949799  1.579137  1.984402  2.493673  3.133642  3.937850   
    4  1.326450  1.063496  1.759470  2.333850  3.095735  4.106339  5.446854   
    
            x_7       x_8        x_9       x_10       x_11       x_12       x_13  \
    0  1.381021  1.446202   1.514459   1.585938   1.660790   1.739176   1.821260   
    1  2.169709  2.423588   2.707173   3.023942   3.377775   3.773011   4.214494   
    2  3.316683  3.936319   4.671717   5.544505   6.580351   7.809718   9.268760   
    3  4.948448  6.218404   7.814277   9.819710  12.339811  15.506664  19.486248   
    4  7.224981  9.583578  12.712139  16.862020  22.366630  29.668222  39.353420   
    
            x_14       x_15  
    0   1.907219   1.997235  
    1   4.707635   5.258479  
    2  11.000386  13.055521  
    3  24.487142  30.771450  
    4  52.200353  69.241170  


Now that we have all the 15 powers, lets make 15 different linear regression models with each model containing variables with powers of x from 1 to the particular model number. For example, the feature set of model 8 will be – {x, x_2, x_3, … ,x_8}.

First, we’ll define a generic function which takes in the required maximum power of x as an input and returns a list containing – [ model RSS, intercept, coef_x, coef_x2, … upto entered power ]. Here RSS refers to ‘Residual Sum of Squares’ which is nothing but the sum of square of errors between the predicted and actual values in the training data set. The python code defining the function is:


```python
#Import Linear Regression model from scikit-learn.
from sklearn.linear_model import LinearRegression
def linear_regression(data, power, models_to_plot):
    #initialize predictors:
    predictors=['x']
    if power>=2:
        predictors.extend(['x_%d'%i for i in range(2,power+1)])
    
    #Fit the model
    linreg = LinearRegression(normalize=True)
    linreg.fit(data[predictors],data['y'])
    y_pred = linreg.predict(data[predictors])
    
    #Check if a plot is to be made for the entered power
    if power in models_to_plot:
        plt.subplot(models_to_plot[power])
        plt.tight_layout()
        plt.plot(data['x'],y_pred)
        plt.plot(data['x'],data['y'],'.')
        plt.title('Plot for power: %d'%power)
    
    #Return the result in pre-defined format
    rss = sum((y_pred-data['y'])**2)
    ret = [rss]
    ret.extend([linreg.intercept_])
    ret.extend(linreg.coef_)
    return ret
```

Note that this function will not plot the model fit for all the powers but will return the **residual sum of squares (RSS)** and coefficients for all the models. I’ll skip the details of the code for now to maintain brevity. I’ll be happy to discuss the same through comments below if required.

Now, we can make all 15 models and compare the results. For ease of analysis, we’ll store all the results in a Pandas dataframe and plot 6 models to get an idea of the trend. Consider the following code:


```python
#Initialize a dataframe to store the results:
col = ['rss','intercept'] + ['coef_x_%d'%i for i in range(1,16)]
ind = ['model_pow_%d'%i for i in range(1,16)]
coef_matrix_simple = pd.DataFrame(index=ind, columns=col)

#Define the powers for which a plot is required:
models_to_plot = {1:231,3:232,6:233,9:234,12:235,15:236}

#Iterate through all powers and assimilate results
for i in range(1,16):
    coef_matrix_simple.iloc[i-1,0:i+2] = linear_regression(data, power=i, models_to_plot=models_to_plot)
```


![png](3.%20Ridge%20Regression_files/3.%20Ridge%20Regression_18_0.png)


We would expect the models with increasing complexity to better fit the data and result in lower RSS values. This can be verified by looking at the plots generated for 6 models.

This clearly aligns with our initial understanding. As the model complexity increases, the models tends to fit even smaller deviations in the training data set. Though this leads to overfitting, lets keep this issue aside for some time and come to our main objective, i.e. the impact on the magnitude of coefficients. This can be analysed by looking at the data frame created above.


```python
#Set the display format to be scientific for ease of analysis
pd.options.display.float_format = '{:,.2g}'.format
coef_matrix_simple
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>rss</th>
      <th>intercept</th>
      <th>coef_x_1</th>
      <th>coef_x_2</th>
      <th>coef_x_3</th>
      <th>coef_x_4</th>
      <th>coef_x_5</th>
      <th>coef_x_6</th>
      <th>coef_x_7</th>
      <th>coef_x_8</th>
      <th>coef_x_9</th>
      <th>coef_x_10</th>
      <th>coef_x_11</th>
      <th>coef_x_12</th>
      <th>coef_x_13</th>
      <th>coef_x_14</th>
      <th>coef_x_15</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>model_pow_1</th>
      <td>3.3</td>
      <td>2</td>
      <td>-0.62</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>model_pow_2</th>
      <td>3.3</td>
      <td>1.9</td>
      <td>-0.58</td>
      <td>-0.006</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>model_pow_3</th>
      <td>1.1</td>
      <td>-1.1</td>
      <td>3</td>
      <td>-1.3</td>
      <td>0.14</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>model_pow_4</th>
      <td>1.1</td>
      <td>-0.27</td>
      <td>1.7</td>
      <td>-0.53</td>
      <td>-0.036</td>
      <td>0.014</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>model_pow_5</th>
      <td>1</td>
      <td>3</td>
      <td>-5.1</td>
      <td>4.7</td>
      <td>-1.9</td>
      <td>0.33</td>
      <td>-0.021</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>model_pow_6</th>
      <td>0.99</td>
      <td>-2.8</td>
      <td>9.5</td>
      <td>-9.7</td>
      <td>5.2</td>
      <td>-1.6</td>
      <td>0.23</td>
      <td>-0.014</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>model_pow_7</th>
      <td>0.93</td>
      <td>19</td>
      <td>-56</td>
      <td>69</td>
      <td>-45</td>
      <td>17</td>
      <td>-3.5</td>
      <td>0.4</td>
      <td>-0.019</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>model_pow_8</th>
      <td>0.92</td>
      <td>43</td>
      <td>-1.4e+02</td>
      <td>1.8e+02</td>
      <td>-1.3e+02</td>
      <td>58</td>
      <td>-15</td>
      <td>2.4</td>
      <td>-0.21</td>
      <td>0.0077</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>model_pow_9</th>
      <td>0.87</td>
      <td>1.7e+02</td>
      <td>-6.1e+02</td>
      <td>9.6e+02</td>
      <td>-8.5e+02</td>
      <td>4.6e+02</td>
      <td>-1.6e+02</td>
      <td>37</td>
      <td>-5.2</td>
      <td>0.42</td>
      <td>-0.015</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>model_pow_10</th>
      <td>0.87</td>
      <td>1.4e+02</td>
      <td>-4.9e+02</td>
      <td>7.3e+02</td>
      <td>-6e+02</td>
      <td>2.9e+02</td>
      <td>-87</td>
      <td>15</td>
      <td>-0.81</td>
      <td>-0.14</td>
      <td>0.026</td>
      <td>-0.0013</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>model_pow_11</th>
      <td>0.87</td>
      <td>-75</td>
      <td>5.1e+02</td>
      <td>-1.3e+03</td>
      <td>1.9e+03</td>
      <td>-1.6e+03</td>
      <td>9.1e+02</td>
      <td>-3.5e+02</td>
      <td>91</td>
      <td>-16</td>
      <td>1.8</td>
      <td>-0.12</td>
      <td>0.0034</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>model_pow_12</th>
      <td>0.87</td>
      <td>-3.4e+02</td>
      <td>1.9e+03</td>
      <td>-4.4e+03</td>
      <td>6e+03</td>
      <td>-5.2e+03</td>
      <td>3.1e+03</td>
      <td>-1.3e+03</td>
      <td>3.8e+02</td>
      <td>-80</td>
      <td>12</td>
      <td>-1.1</td>
      <td>0.062</td>
      <td>-0.0016</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>model_pow_13</th>
      <td>0.86</td>
      <td>3.2e+03</td>
      <td>-1.8e+04</td>
      <td>4.5e+04</td>
      <td>-6.7e+04</td>
      <td>6.6e+04</td>
      <td>-4.6e+04</td>
      <td>2.3e+04</td>
      <td>-8.5e+03</td>
      <td>2.3e+03</td>
      <td>-4.5e+02</td>
      <td>62</td>
      <td>-5.7</td>
      <td>0.31</td>
      <td>-0.0078</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>model_pow_14</th>
      <td>0.79</td>
      <td>2.4e+04</td>
      <td>-1.4e+05</td>
      <td>3.8e+05</td>
      <td>-6.1e+05</td>
      <td>6.6e+05</td>
      <td>-5e+05</td>
      <td>2.8e+05</td>
      <td>-1.2e+05</td>
      <td>3.7e+04</td>
      <td>-8.5e+03</td>
      <td>1.5e+03</td>
      <td>-1.8e+02</td>
      <td>15</td>
      <td>-0.73</td>
      <td>0.017</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>model_pow_15</th>
      <td>0.7</td>
      <td>-3.6e+04</td>
      <td>2.4e+05</td>
      <td>-7.5e+05</td>
      <td>1.4e+06</td>
      <td>-1.7e+06</td>
      <td>1.5e+06</td>
      <td>-1e+06</td>
      <td>5e+05</td>
      <td>-1.9e+05</td>
      <td>5.4e+04</td>
      <td>-1.2e+04</td>
      <td>1.9e+03</td>
      <td>-2.2e+02</td>
      <td>17</td>
      <td>-0.81</td>
      <td>0.018</td>
    </tr>
  </tbody>
</table>
</div>



It is clearly evident that the size of coefficients increase exponentially with increase in model complexity. I hope this gives some intuition into why putting a constraint on the magnitude of coefficients can be a good idea to reduce model complexity.

## Ridge Regression

As mentioned before, ridge regression performs ‘L2 regularization‘, i.e. it adds a factor of sum of squares of coefficients in the optimization objective. Thus, ridge regression optimizes the following:

Objective = RSS + α * (sum of square of coefficients)
Here, α (alpha) is the parameter which balances the amount of emphasis given to minimizing RSS vs minimizing sum of square of coefficients. α can take various values:

- α = 0:
    - The objective becomes same as simple linear regression.
    - We’ll get the same coefficients as simple linear regression.
- α = ∞:
    - The coefficients will be zero. Why? Because of infinite weightage on square of coefficients, anything less than zero will make the objective infinite.
- 0 < α < ∞:
    - The magnitude of α will decide the weightage given to different parts of objective.
    - The coefficients will be somewhere between 0 and ones for simple linear regression.

OR:

It works like linear regression, except that it prevents the polynomial's coefficients to explode. By adding a regularization term in the loss function, ridge regression imposes some structure on the underlying model. 

The ridge regression model has a meta-parameter which represents the weight of the regularization term. We could try different values with trials and errors, using the Ridge class. However, scikit-learn includes another model called RidgeCV which includes a parameter search with cross-validation. In practice, it means that we don't have to tweak this parameter by hand: scikit-learn does it for us. Since the models of scikit-learn always follow the fit-predict API, all we have to do is replace lm.LinearRegression by lm.RidgeCV in the code above. We will give more details in the next section.


```python
import numpy as np
import scipy.stats as st
import sklearn.linear_model as lm
import matplotlib.pyplot as plt
%matplotlib inline

f = lambda x: np.exp(3 * x)
x_tr = np.linspace(0., 2, 200)
y_tr = f(x_tr)

x = np.array([0, .1, .2, .5, .8, .9, 1])
y = f(x) + np.random.randn(len(x))
plt.figure(figsize=(6,3));
plt.plot(x_tr[:100], y_tr[:100], '--k');
plt.plot(x, y, 'ok', ms=10);
```


![png](3.%20Ridge%20Regression_files/3.%20Ridge%20Regression_25_0.png)



```python
# We create the model.
lr = lm.LinearRegression()
# We train the model on our training dataset.
lr.fit(x[:, np.newaxis], y);
# Now, we predict points with our trained model.
y_lr = lr.predict(x_tr[:, np.newaxis])
plt.figure(figsize=(6,3));
plt.plot(x_tr, y_tr, '--k');
plt.plot(x_tr, y_lr, 'g');
plt.plot(x, y, 'ok', ms=10);
plt.xlim(0, 1);
plt.ylim(y.min()-1, y.max()+1);
plt.title("Linear regression");

```


![png](3.%20Ridge%20Regression_files/3.%20Ridge%20Regression_26_0.png)



```python
lrp = lm.LinearRegression()
plt.figure(figsize=(6,3));
plt.plot(x_tr, y_tr, '--k');

for deg, s in zip([2, 5], ['-', '.']):
    lrp.fit(np.vander(x, deg + 1), y);
    y_lrp = lrp.predict(np.vander(x_tr, deg + 1))
    plt.plot(x_tr, y_lrp, s, label='degree ' + str(deg));
    plt.legend(loc=2);
    plt.xlim(0, 1.4);
    plt.ylim(-10, 40);
    # Print the model's coefficients.
    print(' '.join(['%.2f' % c for c in lrp.coef_]))
plt.plot(x, y, 'ok', ms=10);
plt.title("Linear regression");

ridge = lm.RidgeCV()
plt.figure(figsize=(6,3));
plt.plot(x_tr, y_tr, '--k');

for deg, s in zip([2, 5], ['-', '.']):
    ridge.fit(np.vander(x, deg + 1), y);
    y_ridge = ridge.predict(np.vander(x_tr, deg + 1))
    plt.plot(x_tr, y_ridge, s, label='degree ' + str(deg));
    plt.legend(loc=2);
    plt.xlim(0, 1.5);
    plt.ylim(-5, 80);
    # Print the model's coefficients.
    print(' '.join(['%.2f' % c for c in ridge.coef_]))

plt.plot(x, y, 'ok', ms=10);
plt.title("Ridge regression");
```

    20.76 -2.54 0.00
    46.94 -45.56 -26.71 51.04 -6.51 0.00
    11.16 6.41 0.00
    3.21 3.30 3.59 4.18 4.59 0.00



![png](3.%20Ridge%20Regression_files/3.%20Ridge%20Regression_27_1.png)



![png](3.%20Ridge%20Regression_files/3.%20Ridge%20Regression_27_2.png)



```python
from sklearn.linear_model import Ridge

def ridge_regression(data, predictors, alpha, models_to_plot={}):
    #Fit the model
    ridgereg = Ridge(alpha=alpha,normalize=True)
    ridgereg.fit(data[predictors],data['y'])
    y_pred = ridgereg.predict(data[predictors])
    
    #Check if a plot is to be made for the entered alpha
    if alpha in models_to_plot:
        plt.subplot(models_to_plot[alpha])
        plt.tight_layout()
        plt.plot(data['x'],y_pred)
        plt.plot(data['x'],data['y'],'.')
        plt.title('Plot for alpha: %.3g'%alpha)
    
    #Return the result in pre-defined format
    rss = sum((y_pred-data['y'])**2)
    ret = [rss]
    ret.extend([ridgereg.intercept_])
    ret.extend(ridgereg.coef_)
    return ret
```

Note the ‘Ridge’ function used here. It takes ‘alpha’ as a parameter on initialization. Also, keep in mind that normalizing the inputs is generally a good idea in every type of regression and should be used in case of ridge regression as well.

Now, lets analyze the result of Ridge regression for 10 different values of α ranging from 1e-15 to 20. These values have been chosen so that we can easily analyze the trend with change in values of α. These would however differ from case to case.

Note that each of these 10 models will contain all the 15 variables and only the value of alpha would differ. This is different from the simple linear regression case where each model had a subset of features.


```python
#Initialize predictors to be set of 15 powers of x
predictors=['x']
predictors.extend(['x_%d'%i for i in range(2,16)])

#Set the different values of alpha to be tested
alpha_ridge = [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]

#Initialize the dataframe for storing coefficients.
col = ['rss','intercept'] + ['coef_x_%d'%i for i in range(1,16)]
ind = ['alpha_%.2g'%alpha_ridge[i] for i in range(0,10)]
coef_matrix_ridge = pd.DataFrame(index=ind, columns=col)

models_to_plot = {1e-15:231, 1e-10:232, 1e-4:233, 1e-3:234, 1e-2:235, 5:236}
for i in range(10):
    coef_matrix_ridge.iloc[i,] = ridge_regression(data, predictors, alpha_ridge[i], models_to_plot)
```

    /home/mayank/.local/lib64/python3.6/site-packages/scipy/linalg/basic.py:40: RuntimeWarning: scipy.linalg.solve
    Ill-conditioned matrix detected. Result is not guaranteed to be accurate.
    Reciprocal condition number/precision: 3.816092033801343e-17 / 1.1102230246251565e-16
      RuntimeWarning)



![png](3.%20Ridge%20Regression_files/3.%20Ridge%20Regression_30_1.png)


Here we can clearly observe that as the value of alpha increases, the model complexity reduces. Though higher values of alpha reduce overfitting, significantly high values can cause underfitting as well (eg. alpha = 5). Thus alpha should be chosen wisely. A widely accept technique is cross-validation, i.e. the value of alpha is iterated over a range of values and the one giving higher cross-validation score is chosen.


```python
#Set the display format to be scientific for ease of analysis
pd.options.display.float_format = '{:,.2g}'.format
coef_matrix_ridge
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>rss</th>
      <th>intercept</th>
      <th>coef_x_1</th>
      <th>coef_x_2</th>
      <th>coef_x_3</th>
      <th>coef_x_4</th>
      <th>coef_x_5</th>
      <th>coef_x_6</th>
      <th>coef_x_7</th>
      <th>coef_x_8</th>
      <th>coef_x_9</th>
      <th>coef_x_10</th>
      <th>coef_x_11</th>
      <th>coef_x_12</th>
      <th>coef_x_13</th>
      <th>coef_x_14</th>
      <th>coef_x_15</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>alpha_1e-15</th>
      <td>0.87</td>
      <td>94</td>
      <td>-3e+02</td>
      <td>3.8e+02</td>
      <td>-2.4e+02</td>
      <td>68</td>
      <td>-1.2</td>
      <td>-3.7</td>
      <td>0.3</td>
      <td>0.19</td>
      <td>-0.021</td>
      <td>-0.0077</td>
      <td>0.0011</td>
      <td>0.00025</td>
      <td>-6.3e-05</td>
      <td>4.6e-06</td>
      <td>-9e-08</td>
    </tr>
    <tr>
      <th>alpha_1e-10</th>
      <td>0.92</td>
      <td>11</td>
      <td>-29</td>
      <td>31</td>
      <td>-15</td>
      <td>2.9</td>
      <td>0.17</td>
      <td>-0.091</td>
      <td>-0.011</td>
      <td>0.002</td>
      <td>0.00064</td>
      <td>2.4e-05</td>
      <td>-2e-05</td>
      <td>-4.2e-06</td>
      <td>2.2e-07</td>
      <td>2.3e-07</td>
      <td>-2.3e-08</td>
    </tr>
    <tr>
      <th>alpha_1e-08</th>
      <td>0.95</td>
      <td>1.3</td>
      <td>-1.5</td>
      <td>1.7</td>
      <td>-0.68</td>
      <td>0.039</td>
      <td>0.016</td>
      <td>0.00016</td>
      <td>-0.00036</td>
      <td>-5.4e-05</td>
      <td>-2.9e-07</td>
      <td>1.1e-06</td>
      <td>1.9e-07</td>
      <td>2e-08</td>
      <td>3.9e-09</td>
      <td>8.2e-10</td>
      <td>-4.6e-10</td>
    </tr>
    <tr>
      <th>alpha_0.0001</th>
      <td>0.96</td>
      <td>0.56</td>
      <td>0.55</td>
      <td>-0.13</td>
      <td>-0.026</td>
      <td>-0.0028</td>
      <td>-0.00011</td>
      <td>4.1e-05</td>
      <td>1.5e-05</td>
      <td>3.7e-06</td>
      <td>7.4e-07</td>
      <td>1.3e-07</td>
      <td>1.9e-08</td>
      <td>1.9e-09</td>
      <td>-1.3e-10</td>
      <td>-1.5e-10</td>
      <td>-6.2e-11</td>
    </tr>
    <tr>
      <th>alpha_0.001</th>
      <td>1</td>
      <td>0.82</td>
      <td>0.31</td>
      <td>-0.087</td>
      <td>-0.02</td>
      <td>-0.0028</td>
      <td>-0.00022</td>
      <td>1.8e-05</td>
      <td>1.2e-05</td>
      <td>3.4e-06</td>
      <td>7.3e-07</td>
      <td>1.3e-07</td>
      <td>1.9e-08</td>
      <td>1.7e-09</td>
      <td>-1.5e-10</td>
      <td>-1.4e-10</td>
      <td>-5.2e-11</td>
    </tr>
    <tr>
      <th>alpha_0.01</th>
      <td>1.4</td>
      <td>1.3</td>
      <td>-0.088</td>
      <td>-0.052</td>
      <td>-0.01</td>
      <td>-0.0014</td>
      <td>-0.00013</td>
      <td>7.2e-07</td>
      <td>4.1e-06</td>
      <td>1.3e-06</td>
      <td>3e-07</td>
      <td>5.6e-08</td>
      <td>9e-09</td>
      <td>1.1e-09</td>
      <td>4.3e-11</td>
      <td>-3.1e-11</td>
      <td>-1.5e-11</td>
    </tr>
    <tr>
      <th>alpha_1</th>
      <td>5.6</td>
      <td>0.97</td>
      <td>-0.14</td>
      <td>-0.019</td>
      <td>-0.003</td>
      <td>-0.00047</td>
      <td>-7e-05</td>
      <td>-9.9e-06</td>
      <td>-1.3e-06</td>
      <td>-1.4e-07</td>
      <td>-9.3e-09</td>
      <td>1.3e-09</td>
      <td>7.8e-10</td>
      <td>2.4e-10</td>
      <td>6.2e-11</td>
      <td>1.4e-11</td>
      <td>3.2e-12</td>
    </tr>
    <tr>
      <th>alpha_5</th>
      <td>14</td>
      <td>0.55</td>
      <td>-0.059</td>
      <td>-0.0085</td>
      <td>-0.0014</td>
      <td>-0.00024</td>
      <td>-4.1e-05</td>
      <td>-6.9e-06</td>
      <td>-1.1e-06</td>
      <td>-1.9e-07</td>
      <td>-3.1e-08</td>
      <td>-5.1e-09</td>
      <td>-8.2e-10</td>
      <td>-1.3e-10</td>
      <td>-2e-11</td>
      <td>-3e-12</td>
      <td>-4.2e-13</td>
    </tr>
    <tr>
      <th>alpha_10</th>
      <td>18</td>
      <td>0.4</td>
      <td>-0.037</td>
      <td>-0.0055</td>
      <td>-0.00095</td>
      <td>-0.00017</td>
      <td>-3e-05</td>
      <td>-5.2e-06</td>
      <td>-9.2e-07</td>
      <td>-1.6e-07</td>
      <td>-2.9e-08</td>
      <td>-5.1e-09</td>
      <td>-9.1e-10</td>
      <td>-1.6e-10</td>
      <td>-2.9e-11</td>
      <td>-5.1e-12</td>
      <td>-9.1e-13</td>
    </tr>
    <tr>
      <th>alpha_20</th>
      <td>23</td>
      <td>0.28</td>
      <td>-0.022</td>
      <td>-0.0034</td>
      <td>-0.0006</td>
      <td>-0.00011</td>
      <td>-2e-05</td>
      <td>-3.6e-06</td>
      <td>-6.6e-07</td>
      <td>-1.2e-07</td>
      <td>-2.2e-08</td>
      <td>-4e-09</td>
      <td>-7.5e-10</td>
      <td>-1.4e-10</td>
      <td>-2.5e-11</td>
      <td>-4.7e-12</td>
      <td>-8.7e-13</td>
    </tr>
  </tbody>
</table>
</div>



This straight away gives us the following inferences:

- The RSS increases with increase in alpha, this model complexity reduces
- An alpha as small as 1e-15 gives us significant reduction in magnitude of coefficients. How? Compare the coefficients in the first row of this table to the last row of simple linear regression table.
- High alpha values can lead to significant underfitting. Note the rapid increase in RSS for values of alpha greater than 1
- Though the coefficients are very very small, they are NOT zero.
- The first 3 are very intuitive. But #4 is also a crucial observation. Let’s reconfirm the same by determining the number of zeros in each row of the coefficients data set:


```python
    
```


```python
coef_matrix_ridge.apply(lambda x: sum(x.values==0),axis=1)
```




    alpha_1e-15     0
    alpha_1e-10     0
    alpha_1e-08     0
    alpha_0.0001    0
    alpha_0.001     0
    alpha_0.01      0
    alpha_1         0
    alpha_5         0
    alpha_10        0
    alpha_20        0
    dtype: int64



## Lasso Regression

LASSO stands for Least Absolute Shrinkage and Selection Operator. There are 2 key words of interest ‘absolute‘ and ‘selection‘.

Lets consider the former first and worry about the latter later.

Lasso regression performs L1 regularization, i.e. it adds a factor of sum of absolute value of coefficients in the optimization objective. Thus, lasso regression optimizes the following:

Objective = RSS + α * (sum of absolute value of coefficients)

Here, α (alpha) works similar to that of ridge and provides a trade-off between balancing RSS and magnitude of coefficients. Like that of ridge, α can take various values. Lets iterate it here briefly:

- α = 0: Same coefficients as simple linear regression
- α = ∞: All coefficients zero (same logic as before)
- 0 < α < ∞: coefficients between 0 and that of simple linear regression


```python
from sklearn.linear_model import Lasso
def lasso_regression(data, predictors, alpha, models_to_plot={}):
    #Fit the model
    lassoreg = Lasso(alpha=alpha,normalize=True, max_iter=1e5)
    lassoreg.fit(data[predictors],data['y'])
    y_pred = lassoreg.predict(data[predictors])
    
    #Check if a plot is to be made for the entered alpha
    if alpha in models_to_plot:
        plt.subplot(models_to_plot[alpha])
        plt.tight_layout()
        plt.plot(data['x'],y_pred)
        plt.plot(data['x'],data['y'],'.')
        plt.title('Plot for alpha: %.3g'%alpha)
    
    #Return the result in pre-defined format
    rss = sum((y_pred-data['y'])**2)
    ret = [rss]
    ret.extend([lassoreg.intercept_])
    ret.extend(lassoreg.coef_)
    return ret
```


```python
#Initialize predictors to all 15 powers of x
predictors=['x']
predictors.extend(['x_%d'%i for i in range(2,16)])

#Define the alpha values to test
alpha_lasso = [1e-15, 1e-10, 1e-8, 1e-5,1e-4, 1e-3,1e-2, 1, 5, 10]

#Initialize the dataframe to store coefficients
col = ['rss','intercept'] + ['coef_x_%d'%i for i in range(1,16)]
ind = ['alpha_%.2g'%alpha_lasso[i] for i in range(0,10)]
coef_matrix_lasso = pd.DataFrame(index=ind, columns=col)

#Define the models to plot
models_to_plot = {1e-10:231, 1e-5:232,1e-4:233, 1e-3:234, 1e-2:235, 1:236}

#Iterate over the 10 alpha values:
for i in range(10):
    coef_matrix_lasso.iloc[i,] = lasso_regression(data, predictors, alpha_lasso[i], models_to_plot)
```

    /home/mayank/.local/lib64/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.
      ConvergenceWarning)



![png](3.%20Ridge%20Regression_files/3.%20Ridge%20Regression_41_1.png)


This again tells us that the model complexity decreases with increase in the values of alpha. But notice the straight line at alpha=1. Appears a bit strange to me. Let’s explore this further by looking at the coefficients:

### Key Difference

- Ridge: It includes all (or none) of the features in the model. Thus, the major advantage of ridge regression is coefficient shrinkage and reducing model complexity.

- Lasso: Along with shrinking coefficients, lasso performs feature selection as well. (Remember the ‘selection‘ in the lasso full-form?) As we observed earlier, some of the coefficients become exactly zero, which is equivalent to the particular feature being excluded from the model.

### Typical Use Cases

- Ridge: It is majorly used to prevent overfitting. Since it includes all the features, it is not very useful in case of exorbitantly high #features, say in millions, as it will pose computational challenges.
- Lasso: Since it provides sparse solutions, it is generally the model of choice (or some variant of this concept) for modelling cases where the #features are in millions or more. In such a case, getting a sparse solution is of great computational advantage as the features with zero coefficients can simply be ignored.


```python
# URL: http://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_and_elasticnet.html
import numpy as np
import matplotlib.pyplot as plt

from sklearn.metrics import r2_score

# #############################################################################
# Generate some sparse data to play with
np.random.seed(42)

n_samples, n_features = 50, 200
X = np.random.randn(n_samples, n_features)
coef = 3 * np.random.randn(n_features)
inds = np.arange(n_features)
np.random.shuffle(inds)
coef[inds[10:]] = 0  # sparsify coef
y = np.dot(X, coef)

# add noise
y += 0.01 * np.random.normal(size=n_samples)

# Split data in train set and test set
n_samples = X.shape[0]
X_train, y_train = X[:n_samples // 2], y[:n_samples // 2]
X_test, y_test = X[n_samples // 2:], y[n_samples // 2:]

# #############################################################################
# Lasso
from sklearn.linear_model import Lasso

alpha = 0.1
lasso = Lasso(alpha=alpha)

y_pred_lasso = lasso.fit(X_train, y_train).predict(X_test)
r2_score_lasso = r2_score(y_test, y_pred_lasso)
print(lasso)
print("r^2 on test data : %f" % r2_score_lasso)

# #############################################################################
# ElasticNet
from sklearn.linear_model import ElasticNet

enet = ElasticNet(alpha=alpha, l1_ratio=0.7)

y_pred_enet = enet.fit(X_train, y_train).predict(X_test)
r2_score_enet = r2_score(y_test, y_pred_enet)
print(enet)
print("r^2 on test data : %f" % r2_score_enet)

plt.plot(enet.coef_, color='lightgreen', linewidth=2,
         label='Elastic net coefficients')
plt.plot(lasso.coef_, color='gold', linewidth=2,
         label='Lasso coefficients')
plt.plot(coef, '--', color='navy', label='original coefficients')
plt.legend(loc='best')
plt.title("Lasso R^2: %f, Elastic Net R^2: %f"
          % (r2_score_lasso, r2_score_enet))
plt.show()
```

    Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,
       normalize=False, positive=False, precompute=False, random_state=None,
       selection='cyclic', tol=0.0001, warm_start=False)
    r^2 on test data : 0.385982
    ElasticNet(alpha=0.1, copy_X=True, fit_intercept=True, l1_ratio=0.7,
          max_iter=1000, normalize=False, positive=False, precompute=False,
          random_state=None, selection='cyclic', tol=0.0001, warm_start=False)
    r^2 on test data : 0.240498



![png](3.%20Ridge%20Regression_files/3.%20Ridge%20Regression_45_1.png)


## Reference:

- https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/
- http://ipython-books.github.io/featured-04/    
- https://onlinecourses.science.psu.edu/stat857/node/155
- https://machinelearningmastery.com/spot-check-regression-machine-learning-algorithms-python-scikit-learn/
